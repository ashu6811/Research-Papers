{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Section\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import string\n",
    "import gensim\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defination of Functions\n",
    "\n",
    "#to get a list of link from the search query on youtube\n",
    "def load_page(scrape_url, search_hardcode):\n",
    "    mozhdr = {'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-GB; rv:1.9.0.3) Gecko/2008092417 Firefox/3.0.3'}\n",
    "    requests.get(\"https://www.youtube.com\", headers = mozhdr)\n",
    "\n",
    "    sb_url = scrape_url + search_hardcode + \"/\"\n",
    "    print(sb_url)\n",
    "    sb_get = requests.get(sb_url, headers = mozhdr)\n",
    "    sb_get.content\n",
    "    \n",
    "    soupeddata = BeautifulSoup(sb_get.content, \"html.parser\")\n",
    "    yt_links = soupeddata.find_all(\"a\", class_ = \"yt-uix-tile-link\") # might need to change\n",
    "\n",
    "    for x in yt_links:\n",
    "        yt_href = x.get(\"href\")\n",
    "        yt_title = x.get(\"title\")\n",
    "        yt_final = scrape_url + yt_href\n",
    "        print(yt_final)\n",
    "    return soupeddata\n",
    "\n",
    "\n",
    "#Extract title from the soupeddata\n",
    "def get_title(soupeddata):\n",
    "    print (soupeddata.title)\n",
    "    heading = re.findall(r'>([\\w+\\s]+)',str(soupeddata.title))\n",
    "    return heading\n",
    "\n",
    "\n",
    "#Extract title and text data from the soupeddata\n",
    "def get_text(soupeddata):\n",
    "    for script in soupeddata([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    text = soupeddata.get_text()\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    #separating title and converting text_body into list to work on\n",
    "    heading_test = text.splitlines()[0]\n",
    "    text_body = text.splitlines()[1:]\n",
    "    return text_body,heading_test\n",
    "\n",
    "\n",
    "\n",
    "#Start Topic Modeling Process\n",
    "\n",
    "#Clean Doc \n",
    "def clean(doc):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    exclude = set(string.punctuation)\n",
    "    some_more = {'>>', '%' , '$', '/', '#', ':', ',', '.' }\n",
    "    lemma = WordNetLemmatizer()\n",
    "    normalized_stemmed = \"\"\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    some_more_free = ''.join(ch for ch in punc_free if ch not in some_more)\n",
    "    \n",
    "    for word in punc_free.split():\n",
    "        y = lemma.lemmatize(word,\"v\") #lemmatize all words for verb\n",
    "        y = lemma.lemmatize(y,\"a\")    #lemmatize all words for adjectives\n",
    "        y = lemma.lemmatize(y,\"n\")    #lemmatize all words for nouns\n",
    "                           \n",
    "        normalized_stemmed = normalized_stemmed + \" \" + y\n",
    "   \n",
    "    return normalized_stemmed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preparing Document-Term Matrix for text modelling\n",
    "\n",
    "def make_model(doc_clean,n_topics):\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    print(dictionary)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    \n",
    "    # Creating the object for LDA model using gensim library\n",
    "    Lda = gensim.models.ldamodel.LdaModel    \n",
    "    # Running and Trainign LDA model on the document term matrix.\n",
    "    ldamodel = Lda(doc_term_matrix, num_topics=n_topics, id2word = dictionary, passes=50)\n",
    "    return ldamodel\n",
    "\n",
    "#RESULT\n",
    "def pred_title(ldamodel, n_topics, n_words):\n",
    "    headings_pred = ldamodel.print_topics(num_topics=n_topics, num_words=n_words)\n",
    "    all_words = re.findall(r'\"([\\w]+)\"',str(headings_pred))\n",
    "    return all_words\n",
    "\n",
    "def freq(all_words):\n",
    "    frequency = {}\n",
    "    for word in all_words:\n",
    "        count = frequency.get(word,0)   \n",
    "        frequency[word] = count + 1\n",
    "\n",
    "\n",
    "    frequency_list = frequency.keys()\n",
    "\n",
    "    f1 = {}\n",
    "    for k,v in frequency.items():\n",
    "        if(v>2):\n",
    "            f1[k] = v\n",
    "\n",
    "    for k,v in sorted(f1.items(), key=lambda p:p[1], reverse=True):\n",
    "        print(k,v)\n",
    "    \n",
    "\n",
    "\n",
    "# Compare accuracies of predicted topics with the actual one\n",
    "\n",
    "def comp_accu(head_test_split_norm, all_words, stop_words):\n",
    "    accuracy_count =0\n",
    "\n",
    "    for y in head_test_split_norm:\n",
    "        for x in all_words:\n",
    "\n",
    "            if (x == y):\n",
    "                print(y, x)\n",
    "                accuracy_count = accuracy_count+1\n",
    "                break\n",
    "    return accuracy_count\n",
    "\n",
    "def head_count(doc):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    exclude = set(string.punctuation)\n",
    "    some_more = {'>>', '%' , '$', '/', '#', ':', ',', '.' , '?', ':'}\n",
    "    lemma = WordNetLemmatizer()\n",
    "    normalized_stemmed = \"\"\n",
    "    print(doc)\n",
    "    punc_free = \" \".join(ch for ch in doc.lower().split() if ch not in exclude)\n",
    "    print(punc_free)\n",
    "    some_more_free = \" \".join(ch for ch in punc_free.split() if ch not in some_more)\n",
    "    print(some_more_free)\n",
    "    total_count = len(some_more_free.split())\n",
    "    print(total_count)\n",
    "    stop_free = \" \".join([i for i in some_more_free.split() if i not in stop])\n",
    "    print(stop_free)\n",
    "    after_count =len(stop_free.split())\n",
    "    print(after_count)\n",
    "    for word in punc_free.split():\n",
    "        y = lemma.lemmatize(word,\"v\") #lemmatize all words for verb\n",
    "        y = lemma.lemmatize(y,\"a\")    #lemmatize all words for adjectives\n",
    "        y = lemma.lemmatize(y,\"n\")    #lemmatize all words for nouns\n",
    "                           \n",
    "        normalized_stemmed = normalized_stemmed + \" \" + y\n",
    "    temp = (total_count-after_count)\n",
    "    return total_count, after_count, temp\n",
    "    \n",
    "                \n",
    "def show_results(accuracy_count, n_topics, n_words, stop_words):\n",
    "    print(\"\\nAverage Accuracy in\",n_topics,\"topics =\",(accuracy_count/(n_words-stop_words)*100),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.labnol.org/internet/buying-udemy-courses/31851//\n",
      "<title>How to Save Money on Udemy Courses - No Discount Coupons Required</title>\n",
      "How to Save Money on Udemy Courses - No Discount Coupons Required\n",
      "how to save money on udemy courses no discount coupons required\n",
      "how to save money on udemy courses no discount coupons required\n",
      "11\n",
      "save money udemy courses discount coupons required\n",
      "7\n",
      "Dictionary(292 unique tokens: ['tracker', 'overall', 'udemy', 'proceedsavoid', 'ad']...)\n",
      "save save\n",
      "udemy udemy\n",
      "course course\n",
      "\n",
      "Average Accuracy in 5 topics = 42.857142857142854 %\n"
     ]
    }
   ],
   "source": [
    "#MAIN\n",
    "\n",
    "base=\"https://bloggerspassion.com/\"\n",
    "niche1 = \"best-real-estate-affiliate-programs\" \n",
    "niche2 = \"how-to-make-money-online-in-india\"\n",
    "niche3 = \"hostgator-hosting-plans\"\n",
    "niche4 = \"best-fashion-blogs-in-india\"\n",
    "niche5 = \"off-page-seo\"\n",
    "\n",
    "base2 = \"https://www.labnol.org/\"\n",
    "niche6 = \"internet/reliance-jio-phone-ads/31965/\"\n",
    "niche7 = \"internet/gmail-attachment-reminder/31959/\"\n",
    "niche8 = \"internet/101-useful-websites/18078/\"\n",
    "niche9 = \"internet/create-twitter-screenshots/13842/\"\n",
    "niche10 = \"internet/buying-udemy-courses/31851/\"\n",
    "\n",
    "  #number of stop words in title\n",
    "number_of_topics_pred = 5 #number of titles to be predicted for maximum accuracy\n",
    "\n",
    "soupeddata1 = load_page(base2, niche10)\n",
    "blog_title = get_title(soupeddata1)\n",
    "blog_body, heading_test1 = get_text(soupeddata1)\n",
    "number_of_words_in_title, number_of_words_without_stopwords, number_of_stop_words = head_count(heading_test1)\n",
    "cleaned_heading = clean(heading_test1)\n",
    "cleaned_document = [clean(doc).split() for doc in blog_body]\n",
    "lda_model = make_model(cleaned_document, number_of_topics_pred)\n",
    "all_pred_heads = pred_title(lda_model, number_of_topics_pred, number_of_words_in_title)\n",
    "freq(all_pred_heads)\n",
    "accuracy_counts = comp_accu(cleaned_heading.split(), all_pred_heads, number_of_stop_words)\n",
    "\n",
    "#show results\n",
    "show_results(accuracy_counts,number_of_topics_pred, number_of_words_in_title, number_of_stop_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
